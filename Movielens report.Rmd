---
title: "EW Movielens project submission"
author: "Emelie Wahlstedt"
date: "2024-05-16"
output: pdf_document
---

-----------------Start of report------------------

##Introduction/Overview/Executive Summary

**section that describes the dataset and summarizes the goal of the project and key steps that were performed**
This project aims to create a movie recommendation system based on a dataset provided as part of the Data Science: Capstone module, whihc is the final part of the HarvardX Data Science Professional Certificate ptogramme. The dataset provided for this analysis, "movielens", has been divided into a test set ("edx") and a final holdout test set ("final_holdout_test) through code provided as part of the instructions for the project.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We use the following libraries throughout the project:
```{r loading-libs, message=FALSE}
library(tidyverse)
library(caret)
library(stringr)
library(lubridate)
library(dplyr)
library(ggrepel)
library(kknn)
library(randomForest)
library(data.table)
library(speedglm)
library(xgboost)
library(knitr)
```




```{r setup of the edx and final houldout test datasets, echo = FALSE}
##########################################################
# Create edx and final_holdout_test sets 
##########################################################

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
#set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

We now have datasets `r edx` and `r final_holdout_test`. Let's have a look at the `r edx` dataset:

```{r, echo = TRUE}
head(edx)
```

We can see that we have six columns in the dataset, with the rating column being our target variable and the other five being potential predictors. Let's have a closer look at the summary statistics of the dataset:

```{r, overview table creation, echo = FALSE}
#Make an overview table of edx dataset
get_mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

summary_table_edx <- edx %>%
  summarise(
    `Number of ratings` = n(),
    `Number of unique movies` = n_distinct(movieId),
    `Number of unique users` = n_distinct(userId),
    `Number of unique genre combinations` = n_distinct(genres),
    `Mean rating` = mean(rating, na.rm = TRUE),
    `Median rating` = median(rating, na.rm = TRUE),
    `Mode rating` = get_mode(rating)
  )

# Convert the summary table to long format
summary_table_edx_long <- summary_table_edx %>%
  pivot_longer(cols = everything(), names_to = "Statistic", values_to = "Value") %>%
  mutate(Value = case_when(
    Statistic %in% c("Mean rating", "Median rating", "Mode rating") ~ format(round(Value, 1), nsmall = 1), # Set significant digits for mean and median
    TRUE ~ format(round(Value, 0), nsmall = 0) # Set significant digits for other rows
  ))

kable(summary_table_edx_long, caption = "Overview of the edx dataset")
```

We can see that this is a very large dataset - over 9 million rows - which we will need to take into account when we select our model. 

Let's also look at the data quality - do we have any missing values in the dataset? Let's also check the final_holdout_test set to be sure.
```{r fill the missing values with 0, echo = FALSE}
mean(is.na(edx) == "TRUE") # This comes to 0, meaning there are no missing values in our dataset.
mean(is.na(final_holdout_test == "TRUE")) # This also comes to 0.
```

No missing values, great! Let's move on - the genre column in the dataset allows for several genres to be listed for each movie - let's look at which individual genres are present in the dataset:

```{r genre table, echo = FALSE}
# Genre names list
genre_list <- unique(unlist(strsplit(as.character(edx$genres), "\\|")))
kable(genre_list)
length(genre_list)
```

We can see that we have 20 genres, with some movies not having a genre assigned at all - let's check how common this is:

```{r, echo = FALSE}
edx[genres == "(no genres listed)", ]
```

We can see that only one movie has no genres at all, and it has only been rated seven times, so let's leave that alone.

Moving on, let's plot some of the data to get an idea of the distribution of some of the variables. We are going to use average ratings for some plots as plotting the entire dataset will be too visually cluttered and will take a long time.

```{r, echo = FALSE}


```


##Method/Analysis
**section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach**

###Data exploration
The edx dataset is a large dataset of around 9 million rows with six variables. Let's look at a summary of the dataset to gain a broader understanding of what we are looking at:


```{r, echo = FALSE}
summary(edx)

```

We can see that **XXXXXXXXXXXXXXXXX**

Below is an example of the first six rows of the dataset:

```{r, echo = FALSE}
head(edx)

```

From the table above, we can see that the "timestamp" variable which is not in a user friendly format; it is in the "Unix epoch" format which, while it can be read by R without issue, is difficult for most humans to interpret.

We can also see that the movie titles include the year the movie was released, at the end of the movie title in brackets.

Looking at the other variables, "userId", "movieId", and "genres" seems to logically be able to give us information on how a particular movie would be rated by a random user - different users would have different preferences, different movies may have more or less wide appeals, and some genres are probably more popular than others. Hence, these may be important as predictors in the recommendation system model we will build. There may also be some effect of time - for example, if a movie is very new, it may not have had as many ratings yet as there has not been enough time for many users to watch and rate the movie. This could be significant for the average rating that the movie obtains. Let's see if we can see anything of value by plotting some of the variables.

Times movie was rated vs the average rating for the movie

```{r, echo = FALSE}
 edx %>% group_by(movieId) %>% 
   summarise(n_times_rated = n(), avg_rating = mean(rating)) %>% 
   ggplot(aes(avg_rating, n_times_rated)) + 
   geom_point() + 
   labs(x = "Mean rating of movie", y = "Times movie has been rated", title = "Effect of number of ratings on mean rating of movies")
```

Movies appear to have higher ratings the more times they have been rated - which makes sense, as more people watch popular (highly rated) movies.




####Tidying of data

The dataset is large - therefore the format of data.frame makes analysis and modelling slow. Changing the format of the data to data.table significantly improves processing speed, so this is what we do to avoid setting our laptop ablaze.  

In order to be able to better use the information contained in the "timestamp" variable, the year was extracted using the as.datetime() function, resulting in a new variable with the format YYYY. The resulting variable was named rating_year.

In order to be able to use the year of release, this was extracted from the title into a format of YYYY, and named release_year.


####Visual display of data
Before starting the process of building our recommendation system  model, let's

##Finding the optimal model
**Based on the dataset size and structure, the k-nearest neighbours method was selected as it looks at data points with similar attributes to the one that is to be predicted. We also try a random forest model as this can also be useful for recommendation systems.**

##Results

**section that presents the modeling results and discusses the model performance**



##Conclusion

**section that gives a brief summary of the report, its limitations and future work**



